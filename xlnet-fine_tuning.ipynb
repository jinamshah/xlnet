{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env GCE_PROJECT_NAME *insertProjectName*\n",
    "%env TPU_ZONE *insert zone*\n",
    "%env TPU_NAME *insert tpu name*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import xlnet\n",
    "import run_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = '*insert your bucket name*'\n",
    "TASK_VERSION = '*task name*'\n",
    "assert BUCKET, 'Must specify an existing GCS bucket name'\n",
    "OUTPUT_DIR = 'gs://{}/xlnet_large/{}'.format(BUCKET, TASK_VERSION)\n",
    "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XLNET_PRETRAINED_DIR = 'gs://path/to/xlnet model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver([os.environ['TPU_NAME']], zone=os.environ['TPU_ZONE'], project=os.environ['GCE_PROJECT_NAME'])\n",
    "tpu_grpc_url = tpu_cluster_resolver.get_master()\n",
    "\n",
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "    cluster=tpu_cluster_resolver,\n",
    "    model_dir=OUTPUT_DIR,  # define output_dir as the path where you want to store the fine-tuned model\n",
    "    save_checkpoints_steps=1000,\n",
    "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "        iterations_per_loop=1000,\n",
    "        num_shards=8,\n",
    "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
    "\n",
    "xlnet_config = xlnet.XLNetConfig(json_path=os.path.join(XLNET_PRETRAINED_DIR, 'xlnet_config.json'))\n",
    "xlnet_model = run_classifier.get_model_fn(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch sizes can be defined in run_classifier's flags.\n",
    "# i.e. change the default values according to your code as they are required flags\n",
    "num_train_examples, num_dev_examples, num_test_examples = x,y,z   # number of records according to your .tfrecord file\n",
    "num_train_steps = int(\n",
    "    num_train_examples / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "\n",
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "        use_tpu=True,\n",
    "        model_fn=xlnet_model,\n",
    "        config=run_config,\n",
    "        params=None,\n",
    "        train_batch_size=TRAIN_BATCH_SIZE,\n",
    "        predict_batch_size=PREDICT_BATCH_SIZE,\n",
    "        eval_batch_size=EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINE TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = run_classifier.file_based_input_fn_builder(\n",
    "    input_file=\"input tfrecord file\",\n",
    "    seq_length=MAX_SEQ_LENGTH,  # set according the features that you have created\n",
    "    is_training=True,\n",
    "    drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Train the model.\n",
    "print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
    "print('Num examples = {}'.format(num_train_examples))\n",
    "print('Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
    "tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print('***** Finished training at {} *****'.format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('***** Started test eval at {} *****'.format(datetime.datetime.now()))\n",
    "print('  Num examples = {}'.format(num_test_examples))\n",
    "print('  Batch size = {}'.format(EVAL_BATCH_SIZE))\n",
    "test_steps = int(num_test_examples / EVAL_BATCH_SIZE)\n",
    "test_input_fn = run_classifier.file_based_input_fn_builder(\n",
    "    input_file=\"gs://input evaluation tfrecord file\",\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=True)\n",
    "result = estimator.evaluate(input_fn=test_input_fn, steps=test_steps)\n",
    "print('***** Finished evaluation at {} *****'.format(datetime.datetime.now()))\n",
    "output_test_eval_file = os.path.join(OUTPUT_DIR, \"test_results.txt\")\n",
    "with tf.gfile.GFile(output_test_eval_file, \"w\") as writer:\n",
    "  print(\"***** Test Eval results *****\")\n",
    "  for key in sorted(result.keys()):\n",
    "    print('  {} = {}'.format(key, str(result[key])))\n",
    "    writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPORT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "  with tf.variable_scope(\"sorting_hat_sa1_5\"):\n",
    "    feature_spec = {\n",
    "        \"input_ids\": tf.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\n",
    "        \"input_mask\": tf.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\n",
    "        \"segment_ids\": tf.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\n",
    "        \"label_ids\": tf.FixedLenFeature([], tf.int64),\n",
    "      }\n",
    "    serialized_tf_example = tf.placeholder(dtype=tf.string,\n",
    "                                           shape=[None],\n",
    "                                           name='input_example_tensor')\n",
    "    receiver_tensors = {'examples': serialized_tf_example}\n",
    "    features = tf.parse_example(serialized_tf_example, feature_spec)\n",
    "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\n",
    "\n",
    "EXPORT_DIR = 'gs://path/to/export'\n",
    "estimator._export_to_tpu = False  # this is important\n",
    "path = estimator.export_savedmodel(EXPORT_DIR, serving_input_fn)\n",
    "print(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sa_mapping",
   "language": "python",
   "name": "sa_mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
